潜在変数モデルと EM 的学習法
================================================================================

________________________________________________________________________________
はじめに
--------------------------------------------------------------------------------
### 自己紹介
はじめまして！いしはたです！

### 前置き
この記事は Machine Learning Advent Calendar の 12月8日 の記事として書かれています。

### 内容
今日の内容ですが、生成的確率モデリングに関して述べたいと思います。  
具体的には潜在変数（隠れ変数）を含む確率モデルの学習アルゴリズムについて簡単に解説します。

### 想定する読者

対象は「__機械学習はじめてみよう！__」とか「__確率モデルで遊んでみたい！__」
というビギナー向けのつもりです。  
難易度としては高校時代の同級生が「へー」って言ってくれるレベルを目指します。  
場合によってはあえて正確な表現を避け、簡単な言い回しを用いる予定です。  
なので玄人の方が読むと「__何を今更…__」とか「__それは違うだろ！__」
となると思いますがご容赦ください。

________________________________________________________________________________
潜在変数モデル
--------------------------------------------------------------------------------
### 確率モデルと潜在変数
まず「__潜在変数モデル__」というものについて、簡単な実例を交えて簡単に解説します。  
（簡単を繰り替えしてごめんなさい。）  

確率モデルとはざっくり言えば、__同時分布__ です。  
*生成的* 確率モデルとはざっくり言えば、
__データを生成できる（サンプリングできる）__ 確率モデルです。  
潜在変数モデルとはざっくり言えば、生成的確率モデルのうち、  
観測可能である __観測変数 x__ と観測不能である __潜在変数 z__ を持つものです。  
ここでモデルの同時分布を具体的に決めるために、モデルは __パラメータ θ__ を持つとします。  
そしてその同時分布を p(x, z | θ）と書くことにします。

生成的確率モデルの素敵なところは、データの生成過程を想像できるので、解釈しやすいところです。  
潜在変数モデルの素敵なところは、複雑なデータが大量にあっても、  
それらを潜在変数という不思議なものでなんとなく説明してくれるところです。  

潜在変数モデルの使い道としてもっともポピュラーなものがクラスタリングです。  
データがいっぱいあるけど、実はそれらは潜在的にグループに分かれているという解釈です。  
例えば、テレビの視聴履歴を集めたとします。  
すると潜在グループとして、__オタク__ or __リア充__ などが考えられます。  
データはその潜在グループに依存して生成されると考えます。  
例えばオタク野郎はアニメばっかり観てて、リア充様はドラマとかスポーツとか観てるんじゃねえの？(激怒)  

この潜在変数を推定するといろいろ便利なんです。  
例えば Amazon の購入履歴を考えます。  
クラスタリングの結果「こいつらは潜在的に萌豚えだな」と分かれば、
アニメBD売りたい放題とかなるわけです。  

では具体的にはどうやってその潜在変数を推定するのでしょうか。

### 学習と推論
潜在変数モデル p(x, z | θ) では x は既知で、 z と θ は未知とします。  
よって x から z, θ を推定する必要があります。

ここで推定ってなんやねん！となることがあります。  
ここでは単純に z と θ の具体的な値を知りたいという意味で「推定」と言っています。  
しかし個人的なだとzを推定することを推論、θを推定する事を学習と呼ぶ気もします。  
なぜ同じ未知の変数なのに扱いが違うのか。それはそれらが別のタイプの変数だからです。  
潜在変数 z は確率変数であるのに対して、θはパラメータであり、単なる変数なのです。  
前置きはまぁ置いといて推定してみましょう。

#### 準備
一気に両方推定するのは難しそうなので一つづつ推定しましょう。  

まず x, z が既知のとき、θを推定することを考えます。  
これはモデル中の全確率変数が観測可能である (=標本点が一意に定まる) ときに、  
そのパラメータを推定することに対応します。  
このようなデータを完全データと呼びます。  
よってもはや x,z とわけて書かずに x だけで書いてもいいのです。  
完全データからのパラメータの推定は実は中学生くらいでもできます。  
例えばコインを用意し、１００回振ります。そしたら 70 回表、30 回裏でした。  
するとこのコインのパラメータは 表が出る確率 70 % , 裏が出る確率 30 % としたくなります。  
表が55回で裏が45回なら表55%,裏45%としたくなります。__
これは __最尤推定__ と呼ばれる列記とした推定法なのです。
具体的には p(x | θ) を最大化するように θ を決めるのです。  
要するに今起きたことは、『最も尤もらしい出来事』だったと考えるのです。  
最尤推定はまぁ素敵な性質がいっぱいあるのですがここでは割愛します。  

次に x, θ が既知のとき、z を推定することを考えます。  
これは条件付き分布 p(z | x, θ) が既知であるときに z を推定することに対応します。  
先のコインの例で言えば、コインの表裏の出る確率がわかっている時に、  
次のコインの面を推定することに対応します。  
例えば表がでる確率が 99 % で裏の確率が 1 % だったとします。  
次にどっちが出ると思う？と聞かれたらまぁ「表」と答えるでしょう。  
では表55%, 裏45%ではどうでしょう？まぁ一つ選べと言われれば「表」でしょう。  
つまり、p(z |x, θ) を最大化するように z を決めるのです。  
さっきの最尤推定と同じアイディアですね。

しかし、多くの人は 表55%,裏45% の状況で「表！」と叫ぶことを躊躇すると思います。  
それに対し、θを推定するときに表55%,裏45%とすることはあまり抵抗がなかったと思います。  
これは z が確率変数であることを知っているからなのです。  
これがさっき z と θ の推定を推論と学習と言いわけたい気持ちの答えであり、  
同時に今日のテーマのメインでもあります。  

とりあえずこの問題は棚上げし、z,θを同時に推定する方法を考えましょう。  
今、x,zからθを、そしてx,θからzを推定する方法を知っています。  
ではxからz,θを推定するにはどうすればいいか。  
答えは簡単。交互に推定するのです！  

#### MM algorithm (Viterbi Training, K-means etc)



### 各学習法の特徴


________________________________________________________________________________
実験
================================================================================
ここでは実際に紹介した４手法を利用してクラスタリングを行ってみます。

### データ
今回は本をクラスタリングしてみます。  
データは私が趣味で作っている本推薦 bot のデータを利用します。  
bot がフォローしているユーザの本棚中の出現頻度 Top 200 の本を持ってきます。  
次にユーザのうち、それらの本のうち大体半分くらいはもっている人を取ってきます。  
最後にユーザと本の関係を行列で表現します。
縦軸を本、横軸をユーザ、行列の値を

- 0:本を持ってない
- 1:本を持ってる
- 2:本を持ってる、かつ、高評価

の３値で表現します。  
この行列の各列をデータと思って本をクラスタリングしてみます。

### 結果

________________________________________________________________________________
最後に
================================================================================
